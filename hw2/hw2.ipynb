{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 2: Discovery of Frequent Itemsets and Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset: 100000\n"
     ]
    }
   ],
   "source": [
    "# Discovery of Frequent Itemsets and Association Rules\n",
    "\n",
    "# read dataset from T10I4D100K.dat\n",
    "# read intergers sets from dataset line by line\n",
    "def read_dataset(filename):\n",
    "    dataset = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            lines = list(map(int, line.strip().split()))\n",
    "            # lines.sort()\n",
    "            dataset.append(lines)\n",
    "    # dataset.sort()\n",
    "    return dataset\n",
    "\n",
    "dataset = read_dataset('./T10I4D100K.dat')\n",
    "# length of dataset\n",
    "print('length of dataset:', len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implement the A-Priori algorithm for finding frequent itemsets with support at least s in a dataset of sales transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support threshold: 1000\n"
     ]
    }
   ],
   "source": [
    "# get the apporopriate support threshold\n",
    "# typicallly, support threshold is the length of dataset / 100\n",
    "def get_support_threshold(dataset):\n",
    "    return len(dataset) * 0.01\n",
    "\n",
    "print('support threshold:', int(get_support_threshold(dataset)))\n",
    "\n",
    "s = get_support_threshold(dataset)\n",
    "# s = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "220\n",
      "3-itemset: (39, 704, 825) support: 1035\n",
      "[(39, 704, 825)]\n",
      "3\n",
      "0\n",
      "[]\n",
      "len(fre_items): 375\n",
      "fre_items: [25, 52, 240, 274, 368, 448, 538, 561, 630, 687, 775, 825, 834, 39, 120, 205, 401, 581, 704, 814, 35, 674, 733, 854, 950, 422, 449, 857, 895, 937, 964, 229, 283, 294, 381, 708, 738, 766, 853, 883, 966, 978, 104, 143, 569, 620, 798, 185, 214, 350, 529, 658, 682, 782, 809, 947, 970, 227, 390, 71, 192, 208, 279, 280, 496, 530, 597, 618, 675, 720, 914, 932, 183, 217, 276, 653, 706, 878, 161, 175, 177, 424, 490, 571, 623, 795, 910, 960, 125, 130, 392, 461, 862, 27, 78, 900, 921, 147, 411, 572, 579, 778, 803, 266, 290, 458, 523, 614, 888, 944, 43, 70, 204, 334, 480, 513, 874, 151, 504, 890, 73, 310, 419, 469, 722, 810, 844, 846, 918, 967, 326, 403, 526, 774, 788, 789, 975, 116, 198, 201, 171, 541, 701, 805, 946, 471, 487, 631, 638, 678, 735, 780, 935, 17, 242, 758, 763, 956, 145, 385, 676, 790, 792, 885, 522, 617, 859, 12, 296, 354, 548, 684, 740, 841, 210, 346, 477, 605, 829, 884, 234, 460, 649, 746, 600, 28, 157, 5, 115, 517, 736, 744, 919, 196, 489, 494, 641, 673, 362, 591, 31, 58, 181, 472, 573, 628, 651, 111, 154, 168, 580, 632, 832, 871, 988, 72, 981, 10, 132, 21, 32, 54, 239, 348, 100, 500, 48, 126, 319, 639, 765, 521, 112, 140, 285, 387, 511, 594, 93, 583, 606, 236, 952, 90, 593, 941, 122, 718, 1, 423, 516, 6, 69, 797, 913, 577, 110, 509, 611, 995, 343, 527, 33, 336, 989, 97, 574, 793, 598, 427, 470, 37, 992, 55, 897, 275, 51, 259, 45, 162, 378, 534, 906, 912, 576, 373, 716, 546, 665, 963, 349, 8, 197, 413, 749, 823, 94, 982, 984, 515, 692, 694, 567, 57, 800, 812, 41, 414, 923, 377, 752, 991, 998, 899, 710, 867, 170, 438, 563, 357, 332, 361, 322, 928, 75, 486, 440, 38, 784, 265, 686, 540, 468, 663, 819, 886, 429, 843, 129, 578, 510, 68, 860, 4, 887, 309, 804, 325, 826, 394, 707, 105, 815, 948, 308, 661, 634, 351, 405, 688, 949, 163, 893, 335, 173, 258, 85, 450, 428, 550, 769, 554, 366, 820, 207]\n"
     ]
    }
   ],
   "source": [
    "# finding frequent itemsets of integers with support at least s in the dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "class Apriori:\n",
    "    def __init__(self, dataset, support_threshold):\n",
    "        self.dataset = dataset\n",
    "        self.support_threshold = support_threshold\n",
    "        self.frequent_itemsets = []\n",
    "        self.frequent_item = []\n",
    "        self.all_freq_itemsets = []\n",
    "        self.item_frequency = {}\n",
    "        self.frequent_item_sets = []\n",
    "        \n",
    "   # compute the support of given itemset\n",
    "    def support(self, itemset):\n",
    "        support = 0\n",
    "        for transaction in self.dataset:\n",
    "            if set(itemset).issubset(set(transaction)):\n",
    "                support += 1\n",
    "        return support\n",
    "    \n",
    "    # filter generated possible itemset\n",
    "    def filter_itemset(self, itemset):\n",
    "        return (self.support(itemset) >= self.support_threshold)\n",
    "\n",
    "    # get 1-item_set\n",
    "    def get_1_item_set(self):\n",
    "        for transaction in self.dataset:\n",
    "            for item in transaction:\n",
    "                if item not in self.item_frequency:\n",
    "                    self.item_frequency[item] = 1\n",
    "                else:\n",
    "                    self.item_frequency[item] += 1\n",
    "        for item in self.item_frequency:\n",
    "            if self.item_frequency[item] >= self.support_threshold:\n",
    "                self.frequent_item.append(item)\n",
    "        # self.frequent_item.sort()\n",
    "        return self.frequent_item\n",
    "\n",
    "    # update the dataset\n",
    "    def update_dataset(self, itemset):\n",
    "        new_dataset = []\n",
    "        for transaction in self.dataset:\n",
    "            for item in transaction:\n",
    "                if item in itemset:\n",
    "                    new_dataset.append(transaction)\n",
    "                    break\n",
    "        # new_dataset.sort()\n",
    "        self.dataset = new_dataset\n",
    "        return self.dataset\n",
    "    \n",
    "    # get k-item_set\n",
    "    # compute the support using the support function\n",
    "    def get_k_item_set(self, _k_1_itemsets, k):\n",
    "        k_itemsets = []\n",
    "        # get all elements in the _k_1_itemsets\n",
    "        # print(len(_k_1_itemsets))\n",
    "        all_elements = np.hstack(np.array(_k_1_itemsets))\n",
    "        # print(len(all_elements))\n",
    "        # get all unique elements in the _k_1_itemsets\n",
    "        unique_elements = np.unique(all_elements)\n",
    "        print(len(unique_elements))\n",
    "        # print(len(unique_elements))\n",
    "        # get all combinations of unique elements\n",
    "        possible_itemsets = list(itertools.combinations(unique_elements, k))\n",
    "        print(len(possible_itemsets))\n",
    "        # possible_itemsets.sort()\n",
    "        for itemset in possible_itemsets:\n",
    "            if self.filter_itemset(itemset):\n",
    "                k_itemsets.append(itemset)\n",
    "                print(f'{k}-itemset:', itemset, 'support:', self.support(itemset))\n",
    "        return k_itemsets\n",
    "\n",
    "    # get frequent itemset\n",
    "    def get_frequent_itemset(self):\n",
    "        _1_itemsets = self.get_1_item_set()\n",
    "        self.update_dataset(_1_itemsets)\n",
    "        # print(len(_1_itemsets))\n",
    "        self.frequent_itemsets.append(_1_itemsets)\n",
    "        k = 2\n",
    "        while True:\n",
    "            k_itemsets = self.get_k_item_set(_1_itemsets, k)\n",
    "            if len(k_itemsets) == 0:\n",
    "                break\n",
    "            self.frequent_itemsets.append(k_itemsets)\n",
    "            _1_itemsets = k_itemsets\n",
    "            self.update_dataset_by_deleting_infrequent_items(k_itemsets)\n",
    "            k += 1\n",
    "        return self.frequent_itemsets\n",
    "\n",
    "    # update the dataset by deleting the infrequent items\n",
    "    def update_dataset_by_deleting_infrequent_items(self, itemsets):\n",
    "        new_dataset = []\n",
    "        for transaction in self.dataset:\n",
    "            for itemset in itemsets:\n",
    "                if set(itemset).issubset(set(transaction)):\n",
    "                    new_dataset.append(transaction)\n",
    "                    break\n",
    "        self.dataset = new_dataset\n",
    "        return self.dataset\n",
    "    \n",
    "# test\n",
    "apriori = Apriori(dataset, s)\n",
    "# fre_items = apriori.get_frequent_itemset()\n",
    "fre_items = apriori.get_1_item_set()\n",
    "apriori.update_dataset(fre_items)\n",
    "_2_itemsizes = [(39, 704), (39, 825), (217, 346), (227, 390), (368, 682), (368, 829), (390, 722), (704, 825), (789, 829)]\n",
    "apriori.update_dataset_by_deleting_infrequent_items(_2_itemsizes)\n",
    "_3_itemsets = apriori.get_k_item_set(_2_itemsizes, 3)\n",
    "print(_3_itemsets)\n",
    "_4_itemsets = apriori.get_k_item_set(_3_itemsets, 4)\n",
    "print(_4_itemsets)\n",
    "print('len(fre_items):', len(fre_items))\n",
    "print('fre_items:', fre_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((704,), 39, 0.617056856187291), ((39,), 704, 0.259981211836543), ((825,), 39, 0.3847649918962723), ((39,), 825, 0.27876937529356505), ((346,), 217, 0.385014409221902), ((217,), 346, 0.24855813953488373), ((390,), 227, 0.3906890130353817), ((227,), 390, 0.577007700770077), ((682,), 368, 0.2887221684414327), ((368,), 682, 0.15240163515585078), ((829,), 368, 0.17533039647577092), ((368,), 829, 0.15252938170669392), ((722,), 390, 0.17827202737382378), ((390,), 722, 0.3880819366852886), ((825,), 704, 0.35721231766612643), ((704,), 825, 0.6142697881828316), ((829,), 789, 0.17533039647577092), ((789,), 829, 0.27709445346948247), ((704, 825), 39, 0.9392014519056261), ((39, 825), 704, 0.8719460825610783), ((39, 704), 825, 0.9349593495934959)]\n",
      "21\n",
      "(704,) => 39 confidence: 0.617056856187291\n",
      "(39,) => 704 confidence: 0.259981211836543\n",
      "(825,) => 39 confidence: 0.3847649918962723\n",
      "(39,) => 825 confidence: 0.27876937529356505\n",
      "(346,) => 217 confidence: 0.385014409221902\n",
      "(217,) => 346 confidence: 0.24855813953488373\n",
      "(390,) => 227 confidence: 0.3906890130353817\n",
      "(227,) => 390 confidence: 0.577007700770077\n",
      "(682,) => 368 confidence: 0.2887221684414327\n",
      "(368,) => 682 confidence: 0.15240163515585078\n",
      "(829,) => 368 confidence: 0.17533039647577092\n",
      "(368,) => 829 confidence: 0.15252938170669392\n",
      "(722,) => 390 confidence: 0.17827202737382378\n",
      "(390,) => 722 confidence: 0.3880819366852886\n",
      "(825,) => 704 confidence: 0.35721231766612643\n",
      "(704,) => 825 confidence: 0.6142697881828316\n",
      "(829,) => 789 confidence: 0.17533039647577092\n",
      "(789,) => 829 confidence: 0.27709445346948247\n",
      "(704, 825) => 39 confidence: 0.9392014519056261\n",
      "(39, 825) => 704 confidence: 0.8719460825610783\n",
      "(39, 704) => 825 confidence: 0.9349593495934959\n"
     ]
    }
   ],
   "source": [
    "frequent_items = [(39, 704), (39, 825), (217, 346), (227, 390), (368, 682), (368, 829), (390, 722), (704, 825), (789, 829), (39, 704, 825)]\n",
    "# get the association rules\n",
    "def get_association_rules(frequent_itemsets):\n",
    "    rules = []\n",
    "    apriori = Apriori(dataset, s)\n",
    "    for itemset in frequent_itemsets:\n",
    "        for item in itemset:\n",
    "            for item_subset in list(itertools.combinations(itemset, len(itemset) - 1)):\n",
    "                confidence = apriori.support(itemset) / apriori.support(item_subset)\n",
    "                if item not in item_subset:\n",
    "                    rules.append((item_subset, item, confidence))\n",
    "    return rules\n",
    "#test \n",
    "rules = []\n",
    "rules = get_association_rules(frequent_items)\n",
    "print(rules)\n",
    "print(len(rules))\n",
    "# print rules in the form of: item => item confidence\n",
    "for rule in rules:\n",
    "    print(set(rule[0]), '=>', rule[1], 'confidence:', rule[2])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e7551559daae743fd5a6b2b4bfe97d8cf6c753cb1c0fa8928f6e7ddcebdb42f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
