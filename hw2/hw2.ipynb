{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Discovery of Frequent Itemsets and Association Rules\n",
    "\n",
    "### Group 28\n",
    "Junjie Shan & Yuxin Meng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read transactions from dataset\n",
    "\n",
    "Read data from dataset line by line, and turn the string into intergers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset: 100000\n",
      "running time for data reading: 0.0010008811950683594 seconds\n"
     ]
    }
   ],
   "source": [
    "# Discovery of Frequent Itemsets and Association Rules\n",
    "import time\n",
    "# read dataset from T10I4D100K.dat\n",
    "# read intergers sets from dataset line by line\n",
    "t0 = time.time()\n",
    "def read_dataset(filename):\n",
    "    dataset = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            lines = list(map(int, line.strip().split()))\n",
    "            lines.sort()\n",
    "            dataset.append(lines)\n",
    "    dataset.sort()\n",
    "    return dataset\n",
    "t = time.time() - t0\n",
    "dataset = read_dataset('./T10I4D100K.dat')\n",
    "# length of dataset\n",
    "print('length of dataset:', len(dataset))\n",
    "print('running time for data reading:', t, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-1\n",
    "### Implement the A-Priori algorithm\n",
    "The aim of A-Priori algorithm is to find frequent itemsets with support at least s in a dataset of sales transactions.  \n",
    "And the support of an itemset is the number of transactions containing the itemset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the support threshold\n",
    "Usually, we set the support threshold as the length of dataset / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support threshold: 1000\n"
     ]
    }
   ],
   "source": [
    "# get the apporopriate support threshold\n",
    "\n",
    "def get_support_threshold(dataset):\n",
    "    return len(dataset) * 0.01\n",
    "\n",
    "print('support threshold:', int(get_support_threshold(dataset)))\n",
    "\n",
    "s = get_support_threshold(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A-priori Algorithm\n",
    "1. support function & filter_itemset function. we need to calculate the support of an itemset, and then filter out itemset that do not meet the support value greater than or equal to threshold.  \n",
    "2. get_1_item_set function & update_dataset function. In this algorithm, we first need to find individual frequent items and filter out transactions that do not include any frequent items.  \n",
    "3. Then we generated k-itemset from k-1_itemsets, which is called possible itemsets. New added element comes from all single elements contain in k-1_itemsets.  \n",
    "Then calculate the support of each possible itemset, here we don't use support function. The get_k_size_subsets function is used to do this. The elements in the transaction are combined by k and if they are the subset of possible_k_itemsets, it means the transaction contains these frequent itemsets.\n",
    "4. Then update the dataset by removing transactions that don't contain any k-itemsets.\n",
    "5. Repeat 3~4 until the new k_frequent_itemsets is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding frequent itemsets of integers with support at least s in the dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "class Apriori:\n",
    "    def __init__(self, dataset, support_threshold):\n",
    "        self.dataset = dataset\n",
    "        self.support_threshold = support_threshold\n",
    "        self.frequent_itemsets = []\n",
    "        self.frequent_item = []\n",
    "        self.all_freq_itemsets = []\n",
    "        self.item_frequency = {}\n",
    "        self.frequent_item_sets = []\n",
    "        \n",
    "   # compute the support of given itemset\n",
    "    def support(self, itemset):\n",
    "        support = 0\n",
    "        for transaction in self.dataset:\n",
    "            if set(itemset).issubset(set(transaction)):\n",
    "                support += 1\n",
    "        return support\n",
    "    \n",
    "    # filter generated possible itemset\n",
    "    def filter_itemset(self, itemset):\n",
    "        return (self.support(itemset) >= self.support_threshold)\n",
    "\n",
    "    # get 1-item_set\n",
    "    def get_1_item_set(self):\n",
    "        for transaction in self.dataset:\n",
    "            for item in transaction:\n",
    "                if item not in self.item_frequency:\n",
    "                    self.item_frequency[item] = 1\n",
    "                else:\n",
    "                    self.item_frequency[item] += 1\n",
    "        for item in self.item_frequency:\n",
    "            if self.item_frequency[item] >= self.support_threshold:\n",
    "                self.frequent_item.append(item)\n",
    "        # self.frequent_item.sort()\n",
    "        return self.frequent_item\n",
    "\n",
    "    # update the dataset\n",
    "    def update_dataset(self, itemset):\n",
    "        new_dataset = []\n",
    "        for transaction in self.dataset:\n",
    "            for item in transaction:\n",
    "                if item in itemset:\n",
    "                    new_dataset.append(transaction)\n",
    "                    break\n",
    "        # new_dataset.sort()\n",
    "        self.dataset = new_dataset\n",
    "        return self.dataset\n",
    "    \n",
    "    # get all k size subsets contained given itemset of given transactions\n",
    "    def get_k_size_subsets(self, itemset, transactions, k):\n",
    "        return [sets for sets in itertools.combinations(transactions, k) if sets in itemset]\n",
    "    \n",
    "    # get all frequent itemsets\n",
    "    def get_k_item_set(self, _k_1_itemsets, k):\n",
    "        k_itemsets = []\n",
    "        # get all elements in the _k_1_itemsets\n",
    "        all_elements = np.hstack(np.array(_k_1_itemsets))\n",
    "        # get all unique elements in the _k_1_itemsets\n",
    "        unique_elements = np.unique(all_elements)\n",
    "        # get all combinations of unique elements\n",
    "        possible_itemsets = list(itertools.combinations(unique_elements, k))\n",
    "        # convert list to dict and set all values to 0\n",
    "        possible_itemsets = {itemset: 0 for itemset in possible_itemsets}\n",
    "        # get all transactions in the dataset to get the support of each itemset\n",
    "        for transaction in self.dataset:\n",
    "            subsets = self.get_k_size_subsets(possible_itemsets, transaction, k)\n",
    "            for s in subsets:\n",
    "                possible_itemsets[s] += 1\n",
    "        k_itemsets = [item for item in possible_itemsets if possible_itemsets[item] >= self.support_threshold]\n",
    "        # print all frequent itemsets with support\n",
    "        if len(k_itemsets) != 0:\n",
    "            print('frequent itemsets of size', k, ':')\n",
    "            for item in k_itemsets:\n",
    "                print(item, 'support:', possible_itemsets[item])\n",
    "        return k_itemsets\n",
    "\n",
    "    # get frequent itemset\n",
    "    def get_frequent_itemset(self):\n",
    "        _1_itemsets = self.get_1_item_set()\n",
    "        self.update_dataset(_1_itemsets)\n",
    "        # print(len(_1_itemsets))\n",
    "        self.frequent_itemsets.append(_1_itemsets)\n",
    "        k = 2\n",
    "        while True:\n",
    "            k_itemsets = self.get_k_item_set(_1_itemsets, k)\n",
    "            if len(k_itemsets) == 0:\n",
    "                break\n",
    "            self.frequent_itemsets.append(k_itemsets)\n",
    "            _1_itemsets = k_itemsets\n",
    "            self.update_dataset_by_deleting_infrequent_items(k_itemsets)\n",
    "            k += 1\n",
    "        return self.frequent_itemsets\n",
    "\n",
    "    # update the dataset by deleting the infrequent items\n",
    "    def update_dataset_by_deleting_infrequent_items(self, itemsets):\n",
    "        new_dataset = []\n",
    "        for transaction in self.dataset:\n",
    "            for itemset in itemsets:\n",
    "                if set(itemset).issubset(set(transaction)):\n",
    "                    new_dataset.append(transaction)\n",
    "                    break\n",
    "        self.dataset = new_dataset\n",
    "        return self.dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Apriori Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time for 1-itemsets: 0.23600077629089355 seconds\n",
      "{0: 594, 1: 1535, 21: 2666, 48: 2472, 173: 1080, 427: 1856, 529: 7057, 538: 3982, 660: 610, 710: 1044, 731: 560, 829: 6810, 911: 586, 956: 3626, 130: 1711, 698: 236, 839: 854, 53: 535, 168: 1538, 177: 4629, 194: 517, 310: 1390, 350: 3069, 362: 4388, 368: 7828, 442: 326, 600: 1192, 709: 672, 928: 1034, 970: 2086, 54: 2595, 183: 3883, 283: 4082, 944: 2794, 998: 2713, 128: 525, 369: 592, 398: 403, 469: 1502, 496: 1428, 561: 2783, 626: 874, 919: 3710, 471: 2894, 663: 2354, 804: 1315, 871: 2810, 983: 453, 3: 531, 27: 2165, 240: 1399, 274: 2628, 406: 785, 432: 985, 521: 1582, 639: 1572, 692: 4993, 746: 1982, 790: 1094, 855: 939, 879: 865, 73: 2179, 114: 816, 115: 1775, 145: 4559, 166: 346, 279: 3014, 308: 1402, 309: 1262, 508: 95, 618: 1337, 658: 1881, 661: 2693, 665: 1297, 682: 4132, 984: 1756, 4: 1394, 28: 1454, 268: 885, 401: 3667, 430: 580, 530: 1263, 578: 1290, 580: 1667, 755: 392, 913: 1939, 35: 1984, 242: 2325, 317: 266, 392: 2420, 468: 1089, 640: 932, 801: 835, 862: 3649, 865: 396, 55: 1959, 103: 327, 373: 2007, 494: 5102, 541: 3735, 631: 2793, 716: 1199, 722: 5845, 963: 1327, 112: 2680, 275: 1692, 332: 1861, 407: 634, 523: 2244, 766: 6265, 349: 2041, 440: 1943, 634: 2492, 809: 2163, 70: 2411, 212: 778, 217: 5375, 277: 982, 475: 663, 800: 1916, 826: 2022, 833: 629, 895: 3385, 906: 1444, 914: 4037, 950: 1463, 140: 2687, 239: 2742, 257: 132, 480: 2309, 544: 773, 793: 3063, 803: 2237, 5: 1094, 72: 2852, 143: 1417, 351: 1641, 511: 1015, 545: 119, 569: 2835, 606: 2668, 706: 1923, 718: 1238, 732: 799, 749: 1330, 782: 2767, 832: 2062, 841: 1927, 94: 1201, 117: 731, 352: 902, 694: 2847, 736: 1470, 817: 360, 874: 2237, 885: 3043, 357: 1142, 403: 1722, 452: 205, 653: 2634, 708: 1090, 735: 1689, 792: 1306, 886: 3053, 6: 2149, 25: 1395, 31: 1666, 32: 4248, 175: 2791, 188: 650, 304: 805, 387: 2089, 576: 1337, 590: 814, 598: 3219, 630: 1523, 789: 4309, 798: 3103, 843: 1222, 220: 717, 258: 1036, 411: 2047, 422: 1255, 472: 2125, 966: 3921, 818: 323, 758: 2860, 795: 3361, 846: 1480, 51: 1612, 419: 5057, 581: 2943, 752: 2578, 450: 2082, 507: 950, 605: 1652, 825: 3085, 196: 2096, 394: 1145, 674: 2527, 704: 1794, 834: 1373, 884: 1645, 236: 2618, 687: 1762, 785: 947, 797: 2684, 904: 400, 937: 4681, 88: 303, 294: 1445, 381: 2959, 603: 458, 154: 1447, 252: 781, 272: 659, 415: 877, 628: 1102, 676: 2717, 744: 2177, 912: 1009, 589: 214, 720: 3864, 844: 2814, 7: 997, 376: 362, 614: 3134, 225: 371, 449: 1890, 470: 4137, 948: 1149, 8: 3090, 12: 3415, 105: 1100, 684: 5408, 707: 1354, 815: 1358, 867: 1530, 876: 496, 943: 821, 516: 1544, 36: 528, 424: 1448, 925: 652, 981: 1542, 39: 4258, 90: 1875, 230: 288, 41: 1353, 116: 2193, 151: 2611, 208: 1483, 482: 299, 740: 1632, 180: 424, 256: 785, 486: 1547, 69: 2370, 447: 863, 461: 1498, 71: 3507, 214: 1893, 85: 1555, 111: 1171, 155: 697, 348: 1226, 390: 2685, 413: 2637, 96: 975, 533: 772, 594: 1516, 642: 830, 823: 1031, 918: 3012, 80: 826, 346: 3470, 883: 4902, 897: 1935, 458: 1124, 531: 447, 623: 1845, 853: 1804, 10: 1351, 910: 1695, 298: 778, 460: 4438, 608: 999, 493: 583, 573: 1229, 811: 358, 899: 1252, 58: 1330, 334: 2146, 342: 424, 733: 1141, 820: 1473, 857: 1588, 888: 3686, 932: 1786, 941: 1126, 95: 841, 159: 542, 162: 1450, 170: 1203, 192: 2004, 517: 1201, 525: 609, 711: 714, 916: 306, 205: 3605, 673: 1635, 385: 1676, 693: 387, 967: 1695, 57: 2743, 104: 1158, 414: 1160, 935: 1742, 203: 861, 366: 1031, 597: 2883, 688: 1132, 939: 342, 701: 1283, 11: 525, 514: 442, 534: 1531, 686: 1495, 814: 1672, 995: 1521, 15: 458, 52: 1983, 285: 2600, 339: 695, 499: 743, 550: 1203, 579: 2164, 632: 1070, 448: 1370, 522: 2725, 775: 3771, 878: 2047, 887: 1671, 93: 2777, 132: 2641, 181: 1235, 562: 462, 633: 780, 768: 12, 852: 538, 160: 987, 280: 2108, 303: 257, 120: 4973, 164: 744, 213: 522, 638: 2288, 125: 1287, 161: 2320, 675: 2976, 945: 536, 982: 1640, 157: 1140, 227: 1818, 659: 835, 719: 663, 810: 1267, 276: 2479, 757: 245, 851: 577, 978: 1141, 264: 297, 572: 1589, 760: 698, 923: 1753, 78: 2471, 489: 3420, 375: 417, 335: 1345, 487: 3135, 641: 1494, 16: 150, 201: 1029, 358: 365, 856: 109, 17: 1683, 193: 925, 318: 812, 645: 581, 763: 1862, 765: 1705, 819: 1257, 837: 619, 187: 759, 354: 5835, 423: 1412, 548: 2843, 624: 915, 18: 813, 191: 111, 336: 1071, 426: 111, 37: 1249, 456: 804, 234: 1416, 371: 405, 554: 1114, 265: 1359, 295: 712, 812: 1518, 38: 2402, 611: 1444, 44: 903, 374: 490, 780: 2306, 45: 1728, 326: 1488, 520: 472, 715: 381, 989: 1289, 98: 486, 321: 676, 691: 421, 854: 2847, 163: 1256, 751: 397, 198: 1461, 23: 128, 147: 1383, 204: 2174, 395: 990, 794: 624, 973: 512, 24: 191, 915: 241, 43: 1721, 226: 658, 259: 1522, 592: 239, 681: 344, 933: 523, 139: 671, 730: 602, 190: 531, 510: 3281, 890: 1437, 898: 318, 26: 527, 75: 3151, 263: 654, 292: 71, 769: 1622, 947: 3690, 82: 674, 141: 460, 629: 893, 100: 1749, 172: 663, 122: 1081, 723: 829, 845: 585, 900: 1165, 921: 2425, 938: 766, 126: 1075, 171: 1097, 319: 1371, 462: 155, 946: 1350, 647: 551, 712: 845, 893: 1947, 49: 137, 400: 478, 127: 492, 438: 4511, 563: 1065, 567: 1102, 742: 953, 67: 316, 459: 465, 343: 1599, 999: 193, 859: 1242, 595: 797, 721: 442, 602: 463, 672: 743, 355: 958, 620: 2100, 705: 888, 262: 660, 540: 1293, 62: 110, 129: 1547, 593: 2601, 515: 1166, 689: 817, 920: 480, 949: 1414, 33: 1460, 518: 759, 124: 294, 68: 1601, 322: 1154, 405: 1525, 889: 318, 526: 2793, 571: 2902, 952: 1574, 249: 615, 577: 1695, 738: 2129, 113: 394, 179: 237, 377: 1149, 988: 1164, 305: 210, 805: 1789, 307: 702, 753: 578, 975: 1764, 774: 2046, 399: 82, 478: 522, 500: 1444, 574: 1297, 575: 240, 777: 464, 197: 1230, 388: 938, 467: 742, 583: 1389, 762: 792, 509: 3044, 926: 672, 40: 457, 361: 1104, 764: 425, 255: 185, 378: 1149, 176: 582, 513: 1287, 835: 732, 778: 2514, 556: 240, 992: 1116, 813: 422, 66: 888, 648: 653, 745: 909, 934: 510, 46: 543, 247: 181, 266: 1022, 428: 1021, 964: 1518, 622: 826, 788: 2386, 965: 75, 612: 396, 300: 582, 384: 406, 527: 1185, 383: 668, 858: 866, 539: 376, 901: 324, 290: 1793, 512: 611, 662: 114, 969: 849, 210: 2009, 215: 781, 585: 856, 784: 1257, 568: 956, 824: 565, 713: 716, 281: 437, 690: 778, 278: 657, 822: 356, 391: 114, 940: 536, 429: 1037, 909: 525, 408: 607, 892: 220, 960: 2732, 97: 1466, 206: 272, 483: 439, 840: 446, 504: 1296, 830: 841, 560: 382, 894: 407, 991: 1268, 110: 1801, 118: 916, 301: 309, 930: 318, 412: 517, 828: 542, 306: 247, 696: 313, 311: 437, 842: 752, 329: 964, 671: 433, 131: 327, 158: 884, 501: 118, 617: 2614, 185: 1529, 678: 1329, 481: 888, 150: 410, 436: 589, 831: 852, 320: 540, 838: 953, 588: 389, 903: 711, 484: 971, 596: 269, 649: 1292, 174: 579, 241: 784, 178: 654, 953: 525, 421: 656, 860: 1255, 229: 2281, 714: 347, 591: 1241, 327: 662, 490: 1066, 557: 587, 464: 848, 697: 370, 477: 2462, 806: 308, 905: 640, 651: 1288, 652: 354, 927: 156, 444: 260, 64: 319, 296: 2210, 980: 899, 409: 651, 781: 419, 685: 392, 417: 971, 144: 505, 81: 270, 994: 33, 228: 118, 136: 707, 546: 1050, 759: 788, 773: 192, 325: 1022, 750: 446, 616: 568, 267: 223, 455: 453, 108: 940, 156: 572, 207: 1214, 724: 86, 231: 442, 474: 815, 437: 314, 528: 576, 695: 394, 849: 459, 167: 625, 244: 96, 364: 567, 524: 427, 922: 990, 396: 442, 902: 222, 299: 522, 289: 346, 453: 433, 972: 426, 347: 885, 734: 217, 261: 362, 324: 173, 604: 448, 186: 348, 613: 373, 505: 791, 199: 109, 333: 483, 443: 405, 601: 299, 224: 919, 610: 464, 650: 281, 558: 957, 821: 179, 485: 452, 316: 578, 397: 479, 379: 390, 954: 366, 896: 495, 866: 266, 771: 505, 209: 418, 435: 357, 345: 801, 328: 663, 402: 302, 679: 248, 2: 673, 881: 283, 314: 846, 868: 510, 29: 171, 182: 284, 666: 363, 729: 102, 627: 292, 703: 546, 677: 407, 367: 158, 89: 211, 153: 99, 466: 212, 22: 397, 86: 229, 807: 305, 202: 56, 287: 54, 958: 426, 439: 225, 135: 82, 270: 416, 931: 170, 553: 383, 609: 184, 492: 665, 535: 294, 536: 372, 338: 234, 331: 270, 743: 194, 779: 280, 783: 468, 864: 295, 286: 199, 699: 353, 14: 197, 222: 126, 165: 286, 644: 197, 20: 40, 91: 116, 142: 454, 476: 133, 47: 175, 942: 436, 216: 525, 787: 90, 218: 317, 273: 98, 552: 61, 370: 386, 498: 325, 683: 85, 635: 136, 537: 331, 248: 79, 847: 179, 356: 96, 882: 347, 137: 48, 138: 257, 565: 640, 872: 122, 59: 312, 657: 506, 83: 181, 330: 102, 977: 270, 441: 96, 680: 236, 767: 47, 799: 274, 123: 28, 313: 224, 655: 467, 189: 39, 741: 98, 985: 132, 997: 272, 726: 32, 56: 114, 961: 474, 146: 113, 990: 77, 382: 55, 254: 94, 776: 78, 237: 98, 551: 67, 917: 72, 827: 130, 246: 183, 315: 533, 152: 67, 869: 92, 42: 119, 269: 145, 102: 109, 19: 121, 101: 329, 670: 243, 739: 55, 951: 64, 656: 154, 891: 151, 986: 54, 495: 64, 451: 76, 936: 337, 637: 45, 293: 61, 880: 81, 979: 132, 34: 56, 503: 80, 121: 17, 74: 58, 702: 137, 360: 15, 473: 33, 363: 47, 491: 77, 372: 172, 802: 188, 643: 63, 454: 22, 169: 35, 582: 61, 479: 34, 465: 35, 245: 6, 877: 22, 297: 28, 870: 22, 418: 55, 532: 54, 431: 27, 564: 11, 725: 17, 13: 35, 959: 32, 282: 16, 850: 12, 502: 13, 619: 6, 615: 1}\n"
     ]
    }
   ],
   "source": [
    "apriori = Apriori(dataset, s)\n",
    "# print all 1-item_set with support\n",
    "t = time.time()\n",
    "apriori.get_1_item_set()\n",
    "t = time.time() - t\n",
    "print('running time for 1-itemsets:', t, 'seconds')\n",
    "print(apriori.item_frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequent itemsets of size 2 :\n",
      "(39, 704) support: 1107\n",
      "(39, 825) support: 1187\n",
      "(217, 346) support: 1336\n",
      "(227, 390) support: 1049\n",
      "(368, 682) support: 1193\n",
      "(368, 829) support: 1194\n",
      "(390, 722) support: 1042\n",
      "(704, 825) support: 1102\n",
      "(789, 829) support: 1194\n",
      "frequent itemsets of size 3 :\n",
      "(39, 704, 825) support: 1035\n",
      "running time for apriori algorithm for finding larger itemsets: 4.662030458450317 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run Aprori algorithm\n",
    "apriori = Apriori(dataset, s)\n",
    "t1 = time.time()\n",
    "fre_items = apriori.get_frequent_itemset()\n",
    "t2 = time.time()\n",
    "print('running time for apriori algorithm for finding larger itemsets:', t2 - t1, 'seconds')\n",
    "# print(fre_items[1:])\n",
    "# print(fre_items[0], fre_items[1], fre_items[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task-2\n",
    "### Get association rules\n",
    "Develop and implement an algorithm for generating association rules between frequent itemsets discovered by using the A-Priori algorithm in a dataset of sales transactions. The rules must have support at least s and confidence at least c, where s and c are given as input parameters.  \n",
    "\n",
    "confidence((A,B)->C) = support(A,B,C) / support(A,B), if the result larger than or equal to the given confidence, then we keep this association rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequent_items = [(39, 704), (39, 825), (217, 346), (227, 390), (368, 682), (368, 829), (390, 722), (704, 825), (789, 829), (39, 704, 825)]\n",
    "frequent_items = fre_items[1:]\n",
    "apriori_1 = Apriori(dataset, s)\n",
    "apriori_1.update_dataset(apriori.get_1_item_set())\n",
    "apriori_1.update_dataset_by_deleting_infrequent_items(frequent_items[0])\n",
    "# get the association rules\n",
    "def get_association_rules(frequent_itemsets, confidence):\n",
    "    rules = []\n",
    "    for itemset in frequent_itemsets:\n",
    "        for item in itemset:\n",
    "            for item_subset in list(itertools.combinations(itemset, len(itemset) - 1)):\n",
    "                cal_confidence = apriori_1.support(itemset) / apriori_1.support(item_subset)\n",
    "                if item not in item_subset:\n",
    "                    rules.append((item_subset, item, cal_confidence))\n",
    "    rules = [rule for rule in rules if rule[2] >= confidence]\n",
    "    \n",
    "    return rules\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time for finding association rule: 0.5709977149963379 seconds\n",
      "number of rules found: 16\n",
      "{704} => 39 confidence: 0.917910447761194\n",
      "{825} => 39 confidence: 0.8831845238095238\n",
      "{39} => 825 confidence: 0.8376852505292872\n",
      "{346} => 217 confidence: 0.9258489258489259\n",
      "{217} => 346 confidence: 0.8703583061889251\n",
      "{390} => 227 confidence: 0.824685534591195\n",
      "{227} => 390 confidence: 0.9536363636363636\n",
      "{682} => 368 confidence: 0.9120795107033639\n",
      "{722} => 390 confidence: 0.8282988871224165\n",
      "{390} => 722 confidence: 0.8191823899371069\n",
      "{825} => 704 confidence: 0.8199404761904762\n",
      "{704} => 825 confidence: 0.9137645107794361\n",
      "{789} => 829 confidence: 0.9100609756097561\n",
      "{704, 825} => 39 confidence: 0.9392014519056261\n",
      "{825, 39} => 704 confidence: 0.8719460825610783\n",
      "{704, 39} => 825 confidence: 0.9349593495934959\n"
     ]
    }
   ],
   "source": [
    "# Run Association Rule\n",
    "\n",
    "rules = []\n",
    "t1 = time.time()\n",
    "for itemset in frequent_items:\n",
    "    rules.extend(get_association_rules(itemset, 0.8))\n",
    "t2 = time.time()\n",
    "print('running time for finding association rule:', t2 - t1, 'seconds')\n",
    "# print the number of rules\n",
    "print('number of rules found:', len(rules))\n",
    "# print rules in the form of: item => item confidence\n",
    "for rule in rules:\n",
    "    print(set(rule[0]), '=>', rule[1], 'confidence:', rule[2])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e7551559daae743fd5a6b2b4bfe97d8cf6c753cb1c0fa8928f6e7ddcebdb42f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
