{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 2: Discovery of Frequent Itemsets and Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset: 100000\n"
     ]
    }
   ],
   "source": [
    "# Discovery of Frequent Itemsets and Association Rules\n",
    "\n",
    "# read dataset from T10I4D100K.dat\n",
    "# read intergers sets from dataset line by line\n",
    "def read_dataset(filename):\n",
    "    dataset = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            lines = list(map(int, line.strip().split()))\n",
    "            # lines.sort()\n",
    "            dataset.append(lines)\n",
    "    # dataset.sort()\n",
    "    return dataset\n",
    "\n",
    "dataset = read_dataset('./T10I4D100K.dat')\n",
    "# length of dataset\n",
    "print('length of dataset:', len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implement the A-Priori algorithm for finding frequent itemsets with support at least s in a dataset of sales transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support threshold: 1000\n"
     ]
    }
   ],
   "source": [
    "# get the apporopriate support threshold\n",
    "# typicallly, support threshold is the length of dataset / 100\n",
    "def get_support_threshold(dataset):\n",
    "    return len(dataset) * 0.01\n",
    "\n",
    "print('support threshold:', int(get_support_threshold(dataset)))\n",
    "\n",
    "s = get_support_threshold(dataset)\n",
    "# s = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15316/2398557169.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;31m# test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[0mapriori\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mApriori\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m \u001b[0mfre_items\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapriori\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_frequent_itemset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfre_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfre_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15316/2398557169.py\u001b[0m in \u001b[0;36mget_frequent_itemset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0mk_itemsets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_k_item_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_1_itemsets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_itemsets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15316/2398557169.py\u001b[0m in \u001b[0;36mget_k_item_set\u001b[1;34m(self, _k_1_itemsets, k)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# possible_itemsets.sort()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mitemset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossible_itemsets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_itemset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitemset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m                 \u001b[0mk_itemsets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitemset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{k}-itemset:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitemset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'support:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitemset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15316/2398557169.py\u001b[0m in \u001b[0;36mfilter_itemset\u001b[1;34m(self, itemset)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# filter generated possible itemset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfilter_itemset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitemset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitemset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupport_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# get 1-item_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15316/2398557169.py\u001b[0m in \u001b[0;36msupport\u001b[1;34m(self, itemset)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0msupport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtransaction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitemset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missubset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransaction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m                 \u001b[0msupport\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msupport\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# finding frequent itemsets of integers with support at least s in the dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "class Apriori:\n",
    "    def __init__(self, dataset, support_threshold):\n",
    "        self.dataset = dataset\n",
    "        self.support_threshold = support_threshold\n",
    "        self.frequent_itemsets = []\n",
    "        self.frequent_item = []\n",
    "        self.all_freq_itemsets = []\n",
    "        self.item_frequency = {}\n",
    "        self.frequent_item_sets = []\n",
    "        \n",
    "   # compute the support of given itemset\n",
    "    def support(self, itemset):\n",
    "        support = 0\n",
    "        for transaction in self.dataset:\n",
    "            if set(itemset).issubset(set(transaction)):\n",
    "                support += 1\n",
    "        return support\n",
    "    \n",
    "    # filter generated possible itemset\n",
    "    def filter_itemset(self, itemset):\n",
    "        return (self.support(itemset) >= self.support_threshold)\n",
    "\n",
    "    # get 1-item_set\n",
    "    def get_1_item_set(self):\n",
    "        for transaction in self.dataset:\n",
    "            for item in transaction:\n",
    "                if item not in self.item_frequency:\n",
    "                    self.item_frequency[item] = 1\n",
    "                else:\n",
    "                    self.item_frequency[item] += 1\n",
    "        for item in self.item_frequency:\n",
    "            if self.item_frequency[item] >= self.support_threshold:\n",
    "                self.frequent_item.append(item)\n",
    "        # self.frequent_item.sort()\n",
    "        return self.frequent_item\n",
    "\n",
    "    # update the dataset\n",
    "    def update_dataset(self, itemset):\n",
    "        new_dataset = []\n",
    "        for transaction in self.dataset:\n",
    "            for item in transaction:\n",
    "                if item in itemset:\n",
    "                    new_dataset.append(transaction)\n",
    "                    break\n",
    "        # new_dataset.sort()\n",
    "        self.dataset = new_dataset\n",
    "        return self.dataset\n",
    "    \n",
    "    # get all k size subsets contained given itemset of given transactions\n",
    "    def get_k_size_subsets(self, itemset, transactions, k):\n",
    "        return [sets for sets in itertools.combinations(transactions, k) if sets in itemset]\n",
    "    \n",
    "    # get k-item_set\n",
    "    # compute the support using the support function\n",
    "    # def get_k_item_set(self, _k_1_itemsets, k):\n",
    "    #     k_itemsets = []\n",
    "    #     # get all elements in the _k_1_itemsets\n",
    "    #     all_elements = np.hstack(np.array(_k_1_itemsets))\n",
    "    #     # get all unique elements in the _k_1_itemsets\n",
    "    #     unique_elements = np.unique(all_elements)\n",
    "    #     # get all combinations of unique elements\n",
    "    #     possible_itemsets = list(itertools.combinations(unique_elements, k))\n",
    "    #     print(len(possible_itemsets))\n",
    "    #     # possible_itemsets.sort()\n",
    "    #     for itemset in possible_itemsets:\n",
    "    #         if self.filter_itemset(itemset):\n",
    "    #             k_itemsets.append(itemset)\n",
    "    #             print(f'{k}-itemset:', itemset, 'support:', self.support(itemset))\n",
    "    #     return k_itemsets\n",
    "    def get_k_item_set(self, _k_1_itemsets, k):\n",
    "        k_itemsets = []\n",
    "        # get all elements in the _k_1_itemsets\n",
    "        all_elements = np.hstack(np.array(_k_1_itemsets))\n",
    "        # get all unique elements in the _k_1_itemsets\n",
    "        unique_elements = np.unique(all_elements)\n",
    "        # get all combinations of unique elements\n",
    "        possible_itemsets = list(itertools.combinations(unique_elements, k))\n",
    "        # convert list to dict and set all values to 0\n",
    "        possible_itemsets = {itemset: 0 for itemset in possible_itemsets}\n",
    "        # print(len(possible_itemsets))\n",
    "        # possible_itemsets.sort()\n",
    "        for transaction in self.dataset:\n",
    "            subsets = self.get_k_size_subsets(possible_itemsets, transaction, k)\n",
    "            for s in subsets:\n",
    "                possible_itemsets[s] += 1\n",
    "        k_itemsets = [item for item in possible_itemsets if possible_itemsets[item] >= self.support_threshold]\n",
    "        print(f'{k}-itemset:', k_itemsets,)\n",
    "        # for itemset in possible_itemsets:\n",
    "        #     if self.filter_itemset(itemset):\n",
    "        #         k_itemsets.append(itemset)\n",
    "        #         print(f'{k}-itemset:', itemset, 'support:', self.support(itemset))\n",
    "        return k_itemsets\n",
    "\n",
    "    # get frequent itemset\n",
    "    def get_frequent_itemset(self):\n",
    "        _1_itemsets = self.get_1_item_set()\n",
    "        self.update_dataset(_1_itemsets)\n",
    "        # print(len(_1_itemsets))\n",
    "        self.frequent_itemsets.append(_1_itemsets)\n",
    "        k = 2\n",
    "        while True:\n",
    "            k_itemsets = self.get_k_item_set(_1_itemsets, k)\n",
    "            if len(k_itemsets) == 0:\n",
    "                break\n",
    "            self.frequent_itemsets.append(k_itemsets)\n",
    "            _1_itemsets = k_itemsets\n",
    "            self.update_dataset_by_deleting_infrequent_items(k_itemsets)\n",
    "            k += 1\n",
    "        return self.frequent_itemsets\n",
    "\n",
    "    # update the dataset by deleting the infrequent items\n",
    "    def update_dataset_by_deleting_infrequent_items(self, itemsets):\n",
    "        new_dataset = []\n",
    "        for transaction in self.dataset:\n",
    "            for itemset in itemsets:\n",
    "                if set(itemset).issubset(set(transaction)):\n",
    "                    new_dataset.append(transaction)\n",
    "                    break\n",
    "        self.dataset = new_dataset\n",
    "        return self.dataset\n",
    "    \n",
    "# test\n",
    "apriori = Apriori(dataset, s)\n",
    "fre_items = apriori.get_frequent_itemset()\n",
    "print(fre_items)\n",
    "print(len(fre_items))\n",
    "print(fre_items[0], fre_items[1], fre_items[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((704,), 39, 0.617056856187291), ((39,), 704, 0.259981211836543), ((825,), 39, 0.3847649918962723), ((39,), 825, 0.27876937529356505), ((346,), 217, 0.385014409221902), ((217,), 346, 0.24855813953488373), ((390,), 227, 0.3906890130353817), ((227,), 390, 0.577007700770077), ((682,), 368, 0.2887221684414327), ((368,), 682, 0.15240163515585078), ((829,), 368, 0.17533039647577092), ((368,), 829, 0.15252938170669392), ((722,), 390, 0.17827202737382378), ((390,), 722, 0.3880819366852886), ((825,), 704, 0.35721231766612643), ((704,), 825, 0.6142697881828316), ((829,), 789, 0.17533039647577092), ((789,), 829, 0.27709445346948247), ((704, 825), 39, 0.9392014519056261), ((39, 825), 704, 0.8719460825610783), ((39, 704), 825, 0.9349593495934959)]\n",
      "21\n",
      "{704} => 39 confidence: 0.617056856187291\n",
      "{39} => 704 confidence: 0.259981211836543\n",
      "{825} => 39 confidence: 0.3847649918962723\n",
      "{39} => 825 confidence: 0.27876937529356505\n",
      "{346} => 217 confidence: 0.385014409221902\n",
      "{217} => 346 confidence: 0.24855813953488373\n",
      "{390} => 227 confidence: 0.3906890130353817\n",
      "{227} => 390 confidence: 0.577007700770077\n",
      "{682} => 368 confidence: 0.2887221684414327\n",
      "{368} => 682 confidence: 0.15240163515585078\n",
      "{829} => 368 confidence: 0.17533039647577092\n",
      "{368} => 829 confidence: 0.15252938170669392\n",
      "{722} => 390 confidence: 0.17827202737382378\n",
      "{390} => 722 confidence: 0.3880819366852886\n",
      "{825} => 704 confidence: 0.35721231766612643\n",
      "{704} => 825 confidence: 0.6142697881828316\n",
      "{829} => 789 confidence: 0.17533039647577092\n",
      "{789} => 829 confidence: 0.27709445346948247\n",
      "{704, 825} => 39 confidence: 0.9392014519056261\n",
      "{825, 39} => 704 confidence: 0.8719460825610783\n",
      "{704, 39} => 825 confidence: 0.9349593495934959\n"
     ]
    }
   ],
   "source": [
    "# frequent_items = [(39, 704), (39, 825), (217, 346), (227, 390), (368, 682), (368, 829), (390, 722), (704, 825), (789, 829), (39, 704, 825)]\n",
    "frequent_items = fre_items[1:]\n",
    "# get the association rules\n",
    "def get_association_rules(frequent_itemsets):\n",
    "    rules = []\n",
    "    apriori = Apriori(dataset, s)\n",
    "    for itemset in frequent_itemsets:\n",
    "        for item in itemset:\n",
    "            for item_subset in list(itertools.combinations(itemset, len(itemset) - 1)):\n",
    "                confidence = apriori.support(itemset) / apriori.support(item_subset)\n",
    "                if item not in item_subset:\n",
    "                    rules.append((item_subset, item, confidence))\n",
    "    return rules\n",
    "#test \n",
    "rules = []\n",
    "rules = get_association_rules(frequent_items)\n",
    "print(rules)\n",
    "print(len(rules))\n",
    "# print rules in the form of: item => item confidence\n",
    "for rule in rules:\n",
    "    print(set(rule[0]), '=>', rule[1], 'confidence:', rule[2])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e7551559daae743fd5a6b2b4bfe97d8cf6c753cb1c0fa8928f6e7ddcebdb42f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
