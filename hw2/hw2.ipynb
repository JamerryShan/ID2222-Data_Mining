{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 2: Discovery of Frequent Itemsets and Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset: 100000\n"
     ]
    }
   ],
   "source": [
    "# Discovery of Frequent Itemsets and Association Rules\n",
    "\n",
    "# read dataset from T10I4D100K.dat\n",
    "# read intergers sets from dataset line by line\n",
    "def read_dataset(filename):\n",
    "    dataset = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            lines = list(map(int, line.strip().split()))\n",
    "            # lines.sort()\n",
    "            dataset.append(lines)\n",
    "    # dataset.sort()\n",
    "    return dataset\n",
    "\n",
    "dataset = read_dataset('./T10I4D100K.dat')\n",
    "# length of dataset\n",
    "print('length of dataset:', len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implement the A-Priori algorithm for finding frequent itemsets with support at least s in a dataset of sales transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support threshold: 1000\n"
     ]
    }
   ],
   "source": [
    "# get the apporopriate support threshold\n",
    "# typicallly, support threshold is the length of dataset / 100\n",
    "def get_support_threshold(dataset):\n",
    "    return len(dataset) * 0.01\n",
    "\n",
    "print('support threshold:', int(get_support_threshold(dataset)))\n",
    "\n",
    "s = get_support_threshold(dataset)\n",
    "s = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325\n",
      "len(fre_items): 1\n",
      "fre_items: [[368, 39, 120, 937, 283, 766, 883, 529, 682, 914, 217, 177, 419, 722, 789, 145, 354, 684, 829, 460, 494, 362, 32, 470, 692, 438]]\n"
     ]
    }
   ],
   "source": [
    "# finding frequent itemsets of integers with support at least s in the dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "class Apriori:\n",
    "    def __init__(self, dataset, support_threshold):\n",
    "        self.dataset = dataset\n",
    "        self.support_threshold = support_threshold\n",
    "        self.frequent_itemsets = []\n",
    "        self.frequent_item = []\n",
    "        self.all_freq_itemsets = []\n",
    "        self.item_frequency = {}\n",
    "        self.frequent_item_sets = []\n",
    "        \n",
    "   # compute the support of given itemset\n",
    "    def support(self, itemset):\n",
    "        support = 0\n",
    "        for transaction in self.dataset:\n",
    "            if set(itemset).issubset(set(transaction)):\n",
    "                support += 1\n",
    "        return support\n",
    "    \n",
    "    # filter generated possible itemset\n",
    "    def filter_itemset(self, itemset):\n",
    "        return (self.support(itemset) >= self.support_threshold)\n",
    "\n",
    "    # get 1-item_set\n",
    "    def get_1_item_set(self):\n",
    "        for transaction in self.dataset:\n",
    "            for item in transaction:\n",
    "                if item not in self.item_frequency:\n",
    "                    self.item_frequency[item] = 1\n",
    "                else:\n",
    "                    self.item_frequency[item] += 1\n",
    "        for item in self.item_frequency:\n",
    "            if self.item_frequency[item] >= self.support_threshold:\n",
    "                self.frequent_item.append(item)\n",
    "        # self.frequent_item.sort()\n",
    "        return self.frequent_item\n",
    "\n",
    "    # update the dataset\n",
    "    def update_dataset(self, itemset):\n",
    "        new_dataset = []\n",
    "        for transaction in self.dataset:\n",
    "            for item in transaction:\n",
    "                if item in itemset:\n",
    "                    new_dataset.append(transaction)\n",
    "                    break\n",
    "        # new_dataset.sort()\n",
    "        self.dataset = new_dataset\n",
    "        return self.dataset\n",
    "    \n",
    "    # get k-item_set\n",
    "    # compute the support using the support function\n",
    "    def get_k_item_set(self, _k_1_itemsets, k):\n",
    "        k_itemsets = []\n",
    "        # get all elements in the _k_1_itemsets\n",
    "        # print(len(_k_1_itemsets))\n",
    "        all_elements = np.hstack(np.array(_k_1_itemsets))\n",
    "        # print(len(all_elements))\n",
    "        # get all unique elements in the _k_1_itemsets\n",
    "        unique_elements = np.unique(all_elements)\n",
    "        # print(len(unique_elements))\n",
    "        # get all combinations of unique elements\n",
    "        possible_itemsets = list(itertools.combinations(unique_elements, k))\n",
    "        print(len(possible_itemsets))\n",
    "        # possible_itemsets.sort()\n",
    "        for itemset in possible_itemsets:\n",
    "            if self.filter_itemset(itemset):\n",
    "                k_itemsets.append(itemset)\n",
    "                print(f'{k}-itemset:', itemset, 'support:', self.support(itemset))\n",
    "        return k_itemsets\n",
    "\n",
    "    # get frequent itemset\n",
    "    def get_frequent_itemset(self):\n",
    "        _1_itemsets = self.get_1_item_set()\n",
    "        self.update_dataset(_1_itemsets)\n",
    "        # print(len(_1_itemsets))\n",
    "        self.frequent_itemsets.append(_1_itemsets)\n",
    "        k = 2\n",
    "        while True:\n",
    "            k_itemsets = self.get_k_item_set(_1_itemsets, k)\n",
    "            if len(k_itemsets) == 0:\n",
    "                break\n",
    "            self.frequent_itemsets.append(k_itemsets)\n",
    "            _1_itemsets = k_itemsets\n",
    "            self.update_dataset_by_deleting_infrequent_items(k_itemsets)\n",
    "            k += 1\n",
    "        return self.frequent_itemsets\n",
    "\n",
    "    # update the dataset by deleting the infrequent items\n",
    "    def update_dataset_by_deleting_infrequent_items(self, itemsets):\n",
    "        new_dataset = []\n",
    "        for itemset in itemsets:\n",
    "            for transaction in self.dataset:\n",
    "                if set(itemset).issubset(set(transaction)):\n",
    "                    new_dataset.append(transaction)\n",
    "        self.dataset = new_dataset\n",
    "        print(len(self.dataset))\n",
    "        return self.dataset\n",
    "    \n",
    "# test\n",
    "apriori = Apriori(dataset, s)\n",
    "fre_items = apriori.get_frequent_itemset()\n",
    "print('len(fre_items):', len(fre_items))\n",
    "print('fre_items:', fre_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the association rules\n",
    "def get_association_rules(frequent_itemsets):\n",
    "    rules = []\n",
    "    for itemset in frequent_itemsets:\n",
    "        for item in itemset:\n",
    "            for item_subset in list(itertools.combinations(itemset, len(itemset) - 1)):\n",
    "                confidence = apriori.support(itemset) / apriori.support(item_subset)\n",
    "                if confidence > 0.5:\n",
    "                    rules.append((item_subset, item, confidence))\n",
    "    return rules"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e7551559daae743fd5a6b2b4bfe97d8cf6c753cb1c0fa8928f6e7ddcebdb42f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
